# Agent Guide

## Environment Setup

1. **Verify Docker**: Run `docker ps` to check running containers
2. **Build and run lightweight dev Image**: Dockerfile.dev this is made to be lightweight for faster iteration and testing in github codespace. Use this for development and testing before moving to the full image. No GPU acceleration. 
   ```bash
   docker build -f Dockerfile.dev -t .
   docker run -it --rm -v $(pwd):/app <name> /bin/bash
   ```
4. **Container Rule**: DO NOT install python packgages outside of the docker container unless needed to keep the space light. We only have 30 GiG to test. All packages will be build inside the Dockerfile.dev container.
5. The github repository for this project that you are in a git clone of is located here: [gh repo clone sigamani/proj-grounded-telescopes](https://github.com/sigamani/doubleword-technical)

---

## System Diagram

                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚        Users / Clients      â”‚
                       â”‚ (submit batch requests via â”‚
                       â”‚       HTTP POST /start)     â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚       Head Node (CPU)       â”‚
                       â”‚                             â”‚
                       â”‚  FastAPI Gateway            â”‚
                       â”‚  - Receives requests        â”‚
                       â”‚  - Assigns job_id           â”‚
                       â”‚                             â”‚
                       â”‚  Redis DB                   â”‚
                       â”‚  - batch_job_queue          â”‚
                       â”‚  - batch_job_status         â”‚
                       â”‚  - concurrency counter      â”‚
                       â”‚                             â”‚
                       â”‚  SLA & Metrics Tracker      â”‚
                       â”‚  - Tracks ETA per job       â”‚
                       â”‚  - Throughput & tokens/sec  â”‚
                       â”‚  - Alerts if ETA > SLA     â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Job Dispatcher / Worker   â”‚
                       â”‚  Threads (CPU)             â”‚
                       â”‚  - Dequeue jobs from Redis â”‚
                       â”‚  - Respect concurrency      â”‚
                       â”‚  - Submit batch inference   â”‚
                       â”‚    tasks to GPU nodes       â”‚
                       â”‚  - Update SLA & metrics     â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚       GPU Worker Nodes         â”‚
                 â”‚                               â”‚
                 â”‚  Ray Workers                  â”‚
                 â”‚  - vLLM engine (Qwen2.5-0.5B) â”‚
                 â”‚  - Execute batch inference    â”‚
                 â”‚  - Return results to head    â”‚
                 â”‚  - Report tokens processed   â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Batch Output Storage   â”‚
                       â”‚  (S3, local disk, etc.) â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


## Project Requirements

### Functional Requirements
- Build an offline batch inference server using Ray Data and vLLM
- Use the official `ray.data.llm` module with `vLLMEngineProcessorConfig` and `build_llm_processor`
- Use `ds.map_batches` for distributed processing

### Non-Functional Requirements
- Must complete batch jobs within a 24-hour SLA window
- System must be configurable and observable

---

## Workflow Rules

### Phase 1: Read Before Planning
1. Read ALL documents in `@doc`, `@app`, and `@config` directories
2. Review the plan in PLAN (below) and check what has been implemented from that in the code base (test if necessary) and then move to either fixing gaps or moving to the next step
3. 
### Phase 2: Execution
2. If human asks for something that contradicts any of the the previous ways of working or instructions that do not follow good AI development practice, or instructions that will probably derail the main plan then STOP and ask for clarification and confirmation before proceeding. You are doing the user a favour by calling out bad ideas.
3. Cross-check your own planning against these instructions
5. Classes or Functions? As a general rule if you are describing a data object use a class, if you are not use a function. 
6. Keep the function definitions to maximum ten lines of code.
7. Keep function names / class names no more than 15 characters. Do not comment in code, use clear names instead.


---

## Code Quality Standards

### Mandatory API Usage
After EVERY task, verify you are using:
- âœ… `from ray.data.llm import vLLMEngineProcessorConfig, build_llm_processor`
- âœ… `ds.map_batches()` for distributed processing
- âœ… Official Ray Data patterns from: https://docs.ray.io/en/latest/data/batch_inference.html

If you missed any of these, STOP and fix it immediately.

### Forbidden: DO NOT EVER USE Rich Text Emojis in Code
**DELETE** any line containing these emojis in print statements or logs:
- âŒ ğŸš€ âœ… ğŸ“Š âš ï¸ ğŸ’¡ ğŸ”§ ğŸ¯ or ANY other emoji

**Bad Example (DELETE THIS)**:
```python
logger.info("ğŸš€ Starting Ray Data + vLLM Batch Inference Server")
logger.info("âœ… Configuration loaded")
logger.error("âŒ Failed to initialize Ray")
```

**Good Example (USE THIS)**:
```python
logger.info("Starting Ray Data + vLLM Batch Inference Server")
logger.info("Configuration loaded")
logger.error("Failed to initialize Ray")
```

---

## Communication Style

### DO:
- Be methodical, calculating, and calm
- Maintain consistent memory through good organization and bookkeeping
- Reference previous decisions and code when making new changes
- Show your work: explain reasoning before implementing

### DO NOT:
- Write .md summaries or documentation EVER.
- Use emojis in code or logs
- Make changes without verifying against project requirements
- Proceed without reading all context documents first
- Every 3 tasks - perform an audit and springclean for redundant files or code snippets - use the symbiote if needed.
---

## Checklist Before Any Code Changes

```
â–¡ Have I read all documents in @doc, @app, @config?
â–¡ Am I working inside the michaelsigamani/proj-grounded-telescopes container?
â–¡ Am I using ray.data.llm with vLLMEngineProcessorConfig and build_llm_processor?
â–¡ Am I using ds.map_batches for distributed processing?
â–¡ Have I removed all emoji characters from code and logs?
â–¡ Does this change align with the 24-hour SLA requirement?
â–¡ Have I cross-checked against @todo.txt?
```
